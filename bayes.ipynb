{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_csv(filename):\n",
    "    with open(filename, \"r\", newline=\"\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # skip one row\n",
    "        for row in reader:\n",
    "            text, label = row\n",
    "            yield text, label\n",
    "\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for text, label in read_csv(\"input/email_classification.csv\"):\n",
    "    texts.append(text)\n",
    "    labels.append(1 if label == \"spam\" else 0)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.50, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8350515463917525, 0.022004889975550123, 0.5031055900621119]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple, Dict, Iterable, Set\n",
    "import math\n",
    "import string\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "\n",
    "    def __init__(self, k: float = 0.5, min_token_count: int = 10) -> None:\n",
    "        \"\"\"Initializr Naive Bayes model.\n",
    "\n",
    "        :param float k: Smoothing factor, to avoid token probability 0, defaults to 0.5.\n",
    "        :param int min_token_count: Minimal token counts to be used for prediction, defaults to 10.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.min_token_count = min_token_count\n",
    "        self.count_spam: int = 0\n",
    "        self.count_ham: int = 0\n",
    "        self.vocab_counts = defaultdict(int)\n",
    "        self.token_spam_counts = defaultdict(int)\n",
    "        self.token_ham_counts = defaultdict(int)\n",
    "\n",
    "    def tokenize(self, text: str) -> Set[str]:\n",
    "        \"\"\"Returns set of tokens, lowercased and minus punctutation.\"\"\"\n",
    "        text = text.lower()\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "        tokens = text.split()\n",
    "        return set(tokens)\n",
    "\n",
    "    def fit(self, xs: List[str], ys: List[int]) -> None:\n",
    "        \"\"\"Fit training data.\"\"\"\n",
    "        # iterate through each sample\n",
    "        for x, y in zip(xs, ys):\n",
    "            if y == 1:  # if spam\n",
    "                self.count_spam += 1\n",
    "            elif y == 0:  # if ham\n",
    "                self.count_ham += 1\n",
    "\n",
    "            # iterate through each token in sample\n",
    "            x_tokenized = self.tokenize(x)\n",
    "            for token in x_tokenized:\n",
    "                self.vocab_counts[token] += 1\n",
    "                if y == 1:  # if spam\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                elif y == 0:  # if ham\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "    def _proba_single_token(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Returns probability that we see this token in spam and in ham messages.\n",
    "        P(token|spam) : count this token in spam messages / count spam messages\n",
    "        P(token|ham) : count this token in ham messages / count ham messages\n",
    "        \"\"\"\n",
    "        count_token_in_spam = self.token_spam_counts[token]\n",
    "        count_token_in_ham = self.token_ham_counts[token]\n",
    "        p_token_spam = (count_token_in_spam + self.k) / (self.count_spam + self.k * 2)\n",
    "        p_token_ham = (count_token_in_ham + self.k) / (self.count_ham + self.k * 2)\n",
    "        return p_token_spam, p_token_ham\n",
    "\n",
    "    def _predict_single_sample(self, x: str) -> float:\n",
    "        \"\"\"Returns proba that a sample is spam.\"\"\"\n",
    "        x_tokenized = self.tokenize(x)\n",
    "        log_prob_if_spam = 0\n",
    "        log_prob_if_ham = 0\n",
    "        for token, count in self.vocab_counts.items():\n",
    "\n",
    "            # skip rare occuring word\n",
    "            if count < self.min_token_count:\n",
    "                continue\n",
    "\n",
    "            p_token_spam, p_token_ham = self._proba_single_token(token)\n",
    "            # if token appear in text\n",
    "            # add log proba of seeing it\n",
    "            if token in x_tokenized:\n",
    "                log_prob_if_spam += math.log(p_token_spam)\n",
    "                log_prob_if_ham += math.log(p_token_ham)\n",
    "            # if token do not appear in text\n",
    "            # add log proba of not seeing it\n",
    "            elif token not in x_tokenized:\n",
    "                log_prob_if_spam += math.log(1 - p_token_spam)\n",
    "                log_prob_if_ham += math.log(1 - p_token_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "\n",
    "    def predict(self, xs: List[str]) -> List[int]:\n",
    "        \"\"\"Returns list of proba that samples are spam.\"\"\"\n",
    "        return [self._predict_single_sample(x) for x in xs]\n",
    "\n",
    "    def _p_spam_given_token(self, token) -> float:\n",
    "        \"\"\"Returns proba of spam given a token.\"\"\"\n",
    "        p_token_spam, p_token_ham = self._proba_single_token(token)\n",
    "        return p_token_spam / (p_token_spam + p_token_ham)\n",
    "\n",
    "    def word_ranking(self) -> List[str]:\n",
    "        \"\"\"Returns vocab sorted by proba of being in spam message.\"\"\"\n",
    "        words = sorted(\n",
    "            self.vocab_counts.keys(),\n",
    "            key=lambda token: self._p_spam_given_token(token),\n",
    "            reverse=True,\n",
    "        )\n",
    "        return words\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"spam rules\",\n",
    "    \"ham rules\",\n",
    "    \"hello ham\",\n",
    "]\n",
    "labels = [1, 0, 0]\n",
    "\n",
    "bayes = NaiveBayes(k=0.5, min_token_count=0)\n",
    "bayes.fit(texts, labels)\n",
    "\n",
    "assert bayes.vocab_counts.keys() == {\"spam\", \"ham\", \"rules\", \"hello\"}\n",
    "assert bayes.count_spam == 1\n",
    "assert bayes.count_ham == 2\n",
    "assert bayes.token_spam_counts == {\"spam\": 1, \"rules\": 1}\n",
    "assert bayes.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}\n",
    "texts = [\"hello spam\", \"hello ham\", \"okay all\"]\n",
    "bayes.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes = NaiveBayes(k=1, min_token_count=25)\n",
    "bayes.fit(X_train, y_train)\n",
    "y_pred = [int(y > 0.5) for y in bayes.predict(X_test)]\n",
    "\n",
    "acc = np.sum(np.asarray(y_pred) == np.asarray(y_test)) / len(y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 1 , Actual  1: Act now! Limited-time offer on luxury watches. Click here to buy now!\n",
      "Predicted 1 , Actual  1: You're a winner! Click here to claim your exclusive prize.\n",
      "Predicted 1 , Actual  1: You've been selected for a free iPhone X. Click here to claim your prize!\n",
      "Predicted 1 , Actual  1: Get exclusive access to our VIP club. Click here to join now!\n",
      "Predicted 1 , Actual  1: Claim your prize now! Click here to confirm your winnings.\n",
      "Predicted 1 , Actual  1: Congratulations! You've been selected for a job interview. Click here to schedule your interview.\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(zip(X_test, y_test)):\n",
    "    pred = bayes.predict([x])[0] > 0.5\n",
    "    print(f\"Predicted {pred: <1} , Actual {y: < 1}: {x}\")\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['click',\n",
       " 'claim',\n",
       " 'here',\n",
       " 'rich',\n",
       " 'prize',\n",
       " 'won',\n",
       " 'youve',\n",
       " 'suspended',\n",
       " 'quick',\n",
       " 'join']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_spam = bayes.word_ranking()[:10]\n",
    "top10_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'were',\n",
       " 'is',\n",
       " 'thank',\n",
       " 'heres',\n",
       " 'latest',\n",
       " 'excited',\n",
       " 'purchase',\n",
       " 'team',\n",
       " 'us']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_ham = bayes.word_ranking()[-10:]\n",
    "top10_ham.reverse()\n",
    "top10_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does that tokenization works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'between',\n",
       " 'datasciencester',\n",
       " 'investigating',\n",
       " 'recall',\n",
       " 'relationship',\n",
       " 'that',\n",
       " 'the',\n",
       " 'we',\n",
       " 'were'}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Recall that we were investigating the relationship between a DataSciencester...\"\n",
    "bayes.tokenize(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
